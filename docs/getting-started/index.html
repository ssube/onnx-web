<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><link rel="canonical" href="https://onnx-web.ai/docs/getting-started/" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Getting Started - onnx-web docs</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Getting Started";
        var mkdocs_page_input_path = "getting-started.md";
        var mkdocs_page_url = "/docs/getting-started/";
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> onnx-web docs
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">onnx-web</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../api/">API</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../architecture/">Architecture</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../chain-pipelines/">Chain Pipelines</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../compatibility/">Compatibility</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../converting-models/">Converting Models</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../dev-test/">Development and Testing</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="./">Getting Started</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#contents">Contents</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#setup">Setup</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#windows-bundle-setup">Windows bundle setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#cross-platform-setup">Cross platform setup</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#server-setup-with-containers">Server setup with containers</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#running">Running</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#running-the-server">Running the server</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#running-the-web-ui">Running the web UI</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#tabs">Tabs</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#txt2img-tab">Txt2img Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#img2img-tab">Img2img Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#inpaint-tab">Inpaint Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#upscale-tab">Upscale Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#blend-tab">Blend Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#models-tab">Models Tab</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#settings-tab">Settings Tab</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#image-parameters">Image parameters</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#common-image-parameters">Common image parameters</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#unique-image-parameters">Unique image parameters</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#prompt-syntax">Prompt syntax</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#loras-and-embeddings">LoRAs and embeddings</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#clip-skip">CLIP skip</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#highres">Highres</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#highres-prompt">Highres prompt</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#highres-iterations">Highres iterations</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#profiles">Profiles</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#loading-from-files">Loading from files</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#saving-profiles-in-the-web-ui">Saving profiles in the web UI</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#sharing-parameters-profiles">Sharing parameters profiles</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#panorama-pipeline">Panorama pipeline</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#region-prompts">Region prompts</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#region-seeds">Region seeds</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#grid-mode">Grid mode</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#grid-tokens">Grid tokens</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#memory-optimizations">Memory optimizations</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#converting-to-fp16">Converting to fp16</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#moving-models-to-the-cpu">Moving models to the CPU</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#adding-your-own-models">Adding your own models</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#editing-the-extras-file">Editing the extras file</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#more-details">More details</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../server-admin/">Server Administration</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../setup-guide/">Setup Guide</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../user-guide/">User Guide</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">onnx-web docs</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Getting Started</li>
    <li class="wy-breadcrumbs-aside">
          <a href="https://github.com/ssube/onnx-web/edit/master/docs/getting-started.md" class="icon icon-github"> Edit on GitHub</a>
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="getting-started">Getting Started</h1>
<p>Welcome to onnx-web, a Stable Diffusion tool designed for straightforward and versatile use in AI art. Whether you're
running on an AMD or Nvidia GPU, on a Windows desktop or a Linux server, onnx-web is compatible with most machines.
It goes a step further by supporting multiple GPUs simultaneously, and with SDXL and LCM available for all platforms,
users can harness its capabilities without waiting. The panorama pipeline allows you to use regional prompts without
the need for additional plugins. Get ready to explore the basics of onnx-web and discover how it seamlessly fits into
your AI art toolkit.</p>
<p>The specialized features within onnx-web, such as the panorama pipeline and highres, are designed to produce
high-quality results in the megapixel scale from text prompts without human intervention or inpainting. The panorama
pipeline allows for the generation of large and seamless images, enhanced by the utilization of region prompts and
seeds. Highres, on the other hand, provides a super-resolution upscaling technique, refining images with iterative
img2img processes. Learn how to harness these features effectively, understanding the nuances of region-based
modifications, tokens, and optimal tile configurations.</p>
<p>Discover how onnx-web caters to users with varying hardware configurations. Uncover optimization techniques such as
converting models to fp16 mode, offloading computations to the CPU, and leveraging specialized features like the
panorama pipeline to further reduce memory usage. Gain insights into the considerations and trade-offs involved in
choosing parameters, tile configurations, and employing unique prompts for distinct creative outcomes.</p>
<p>This guide sets the stage for your onnx-web journey, offering a balance of technical depth and user-friendly insights to
empower you in your AI art exploration. Let's embark on this creative venture together, where innovation meets technical
precision.</p>
<h2 id="contents">Contents</h2>
<ul>
<li><a href="#getting-started">Getting Started</a><ul>
<li><a href="#contents">Contents</a></li>
<li><a href="#setup">Setup</a><ul>
<li><a href="#windows-bundle-setup">Windows bundle setup</a></li>
<li><a href="#cross-platform-setup">Cross platform setup</a></li>
<li><a href="#server-setup-with-containers">Server setup with containers</a></li>
</ul>
</li>
<li><a href="#running">Running</a><ul>
<li><a href="#running-the-server">Running the server</a></li>
<li><a href="#running-the-web-ui">Running the web UI</a></li>
</ul>
</li>
<li><a href="#tabs">Tabs</a><ul>
<li><a href="#txt2img-tab">Txt2img Tab</a></li>
<li><a href="#img2img-tab">Img2img Tab</a></li>
<li><a href="#inpaint-tab">Inpaint Tab</a></li>
<li><a href="#upscale-tab">Upscale Tab</a></li>
<li><a href="#blend-tab">Blend Tab</a></li>
<li><a href="#models-tab">Models Tab</a></li>
<li><a href="#settings-tab">Settings Tab</a></li>
</ul>
</li>
<li><a href="#image-parameters">Image parameters</a><ul>
<li><a href="#common-image-parameters">Common image parameters</a></li>
<li><a href="#unique-image-parameters">Unique image parameters</a></li>
</ul>
</li>
<li><a href="#prompt-syntax">Prompt syntax</a><ul>
<li><a href="#loras-and-embeddings">LoRAs and embeddings</a></li>
<li><a href="#clip-skip">CLIP skip</a></li>
</ul>
</li>
<li><a href="#highres">Highres</a><ul>
<li><a href="#highres-prompt">Highres prompt</a></li>
<li><a href="#highres-iterations">Highres iterations</a></li>
</ul>
</li>
<li><a href="#profiles">Profiles</a><ul>
<li><a href="#loading-from-files">Loading from files</a></li>
<li><a href="#saving-profiles-in-the-web-ui">Saving profiles in the web UI</a></li>
<li><a href="#sharing-parameters-profiles">Sharing parameters profiles</a></li>
</ul>
</li>
<li><a href="#panorama-pipeline">Panorama pipeline</a><ul>
<li><a href="#region-prompts">Region prompts</a></li>
<li><a href="#region-seeds">Region seeds</a></li>
</ul>
</li>
<li><a href="#grid-mode">Grid mode</a><ul>
<li><a href="#grid-tokens">Grid tokens</a></li>
</ul>
</li>
<li><a href="#memory-optimizations">Memory optimizations</a><ul>
<li><a href="#converting-to-fp16">Converting to fp16</a></li>
<li><a href="#moving-models-to-the-cpu">Moving models to the CPU</a></li>
</ul>
</li>
<li><a href="#adding-your-own-models">Adding your own models</a><ul>
<li><a href="#editing-the-extras-file">Editing the extras file</a></li>
</ul>
</li>
<li><a href="#more-details">More details</a></li>
</ul>
</li>
</ul>
<h2 id="setup">Setup</h2>
<p>Before diving into the creative process, it's crucial to get onnx-web up and running on your system. We offer multiple
installation methods to cater to users of varying technical expertise. For beginners on Windows, an all-in-one EXE
bundle simplifies the installation process. Intermediate users can opt for a cross-platform installation using a Python
virtual environment, providing greater customization. Server administrators can explore OCI containers for streamlined
deployment. Keep in mind the minimum and recommended system requirements to ensure optimal performance, with options for
optimizations tailored to low-memory users.</p>
<h3 id="windows-bundle-setup">Windows bundle setup</h3>
<p>For Windows users, the all-in-one EXE bundle is an accessible option. There are some minor limitations, for example you
cannot update dependencies or use the nightly ONNX packages, or use plugins that are not included in the bundle.</p>
<p>This bundle, distributed as a ZIP archive, requires extraction before execution. Make sure to disable the Windows
mark-of-the-web check to ensure the server runs successfully. Upon extraction, initiate onnx-web by running the EXE
file. Once model conversion is complete, the server will commence, opening a browser window for immediate access to the
web UI.</p>
<p>For more details, please see <a href="../setup-guide/#windows-all-in-one-bundle">the setup guide</a>.</p>
<h3 id="cross-platform-setup">Cross platform setup</h3>
<p>Users who have a working Python environment and prefer installing their own dependencies can opt for a cross-platform
installation using a Python virtual environment. Ensure a functioning Python setup with either pip or conda. Begin by
cloning the onnx-web git repository, followed by installing the base requirements and those specific to your GPU
platform. Execute the launch script, patiently waiting for the model conversion process to conclude. Post-conversion,
open your browser and load the web UI for interaction.</p>
<p>For more details, please see <a href="../setup-guide/#cross-platform-method">the setup guide</a>.</p>
<h3 id="server-setup-with-containers">Server setup with containers</h3>
<p>For server administrators, onnx-web is also distributed as OCI containers, offering a streamlined deployment. Choose the
installation method that aligns with your proficiency and system requirements, whether it's the beginner-friendly EXE
bundle, the intermediate cross-platform setup, or the containerized deployment for server admins. Each pathway ensures a
straightforward onnx-web installation tailored to your technical needs.</p>
<p>For more details, please see <a href="../server-admin/#containers">the server admin guide</a>.</p>
<h2 id="running">Running</h2>
<p>Running onnx-web is the gateway to unlocking the creative potential of Stable Diffusion in AI art. Whether you are a
novice or an experienced enthusiast, this section guides you through the process of installing onnx-web on your system.</p>
<h3 id="running-the-server">Running the server</h3>
<p>Initiate onnx-web by launching the Python server application, a process that demands your attention before proceeding
further. Specifically, ensure that your models are converted before the server commences its operation. For Windows
users, the distribution is available in the form of a convenient EXE bundle. Allow the server the necessary time to
perform this crucial model conversion, a prerequisite for optimal functionality during your creative sessions. Exercise
patience during this stage to guarantee a smooth experience with onnx-web.</p>
<h3 id="running-the-web-ui">Running the web UI</h3>
<p>After the server has successfully completed its startup procedures, access the web UI through your chosen evergreen
browser, including mobile browsers. Use the same URL and local port as the server to establish the essential connection.
This step solidifies the link between the server and the user interface, unlocking the full capabilities of onnx-web.
For those seeking additional functionality, explore ControlNet using your phone camera directly through the web UI. With
these meticulous steps completed, you are now equipped to efficiently harness the power of onnx-web and navigate the
realm of Stable Diffusion without compromise.</p>
<h2 id="tabs">Tabs</h2>
<p>Understanding onnx-web's Core Features: onnx-web introduces a set of core features that form the backbone of your AI art
journey. The Stable Diffusion process, capable of running on both AMD and Nvidia GPUs, powers the image generation
pipeline. Explore the diverse tabs in the web UI, each offering unique functionalities such as text-based image
generation, upscaling, blending, and model management. Dive into the technical details of prompt syntax, model
conversions, and the intricacies of parameters, gaining a deeper understanding of how to fine-tune the AI art creation
process.</p>
<h3 id="txt2img-tab">Txt2img Tab</h3>
<p>The txt2img tab in onnx-web serves the purpose of generating images from text prompts. Users can input textual
descriptions and witness the algorithm's creative interpretation, providing a seamless avenue for text-based image
generation.</p>
<p>For more details, please see <a href="../user-guide/#txt2img-tab">the user guide</a>.</p>
<h3 id="img2img-tab">Img2img Tab</h3>
<p>For image-based prompts, the img2img tab is the go-to interface within onnx-web. Beyond its fundamental image
generation capabilities, this tab introduces the ControlNet mode, empowering users with advanced control over the
generated images through an innovative feature set.</p>
<p>For more details, please see <a href="../user-guide/#img2img-tab">the user guide</a>.</p>
<h3 id="inpaint-tab">Inpaint Tab</h3>
<p>The inpaint tab specializes in image generation with a unique combination of image prompts and masks. This
functionality allows users to guide the algorithm using both the source image and a mask, enhancing the precision and
customization of the generated content.</p>
<p>For more details, please see <a href="../user-guide/#inpaint-tab">the user guide</a>.</p>
<h3 id="upscale-tab">Upscale Tab</h3>
<p>Addressing the need for higher resolutions, the upscale tab provides users with tools for high resolution and super
resolution. This feature is particularly useful for enhancing the quality and clarity of generated images, meeting
diverse artistic and practical requirements.</p>
<p>For more details, please see <a href="../user-guide/#upscale-tab">the user guide</a>.</p>
<h3 id="blend-tab">Blend Tab</h3>
<p>Enabling users to combine outputs or integrate external images, the blend tab in onnx-web offers a versatile blending
tool. This functionality adds a layer of creativity by allowing users to merge multiple outputs or incorporate
external images seamlessly.</p>
<p>For more details, please see <a href="../user-guide/#blend-tab">the user guide</a>.</p>
<h3 id="models-tab">Models Tab</h3>
<p>Central to managing the core of onnx-web, the models tab provides users with the capability to manage Stable Diffusion
models. Additionally, it allows for the management of LoRAs (Latents of Random Ancestors) associated with these
models, facilitating a comprehensive approach to model customization.</p>
<p>For more details, please see <a href="../user-guide/#models-tab">the user guide</a>.</p>
<h3 id="settings-tab">Settings Tab</h3>
<p>Tailoring the user experience, the settings tab is the control center for managing onnx-web's web UI settings. Users
can configure server APIs, toggle dark mode for a personalized visual experience, and reset other tabs as needed,
ensuring a user-friendly and customizable environment.</p>
<p>For more details, please see <a href="../user-guide/#settings-tab">the user guide</a>.</p>
<h2 id="image-parameters">Image parameters</h2>
<p>In onnx-web, image parameters play a pivotal role in shaping the output of the Stable Diffusion process. These
parameters, including scheduler, CFG, steps, seed, batch size, prompt, optional negative prompt, and image width and
height, collectively govern the characteristics of the diffusion model's training and the resulting generated images.</p>
<h3 id="common-image-parameters">Common image parameters</h3>
<p>These parameters are part of the Stable Diffusion pipeline and common to most tools.</p>
<ul>
<li>Scheduler<ul>
<li>Role: The scheduler dictates the annealing schedule during the diffusion process.</li>
<li>Explanation: It determines how the noise level changes over time, influencing how the diffusion process resolves
  complex features like faces. Some schedulers are faster than others and some are more deterministic, reliably
  reproducing the same results.</li>
<li>LCM and Turbo require specific schedulers, which are marked in the web UI.</li>
</ul>
</li>
<li>Eta<ul>
<li>Only applies to the DDIMScheduler, and is ignored in other schedulers.</li>
<li>A value of 0 corresponds to DDIM and 1 corresponds to DDPM.</li>
</ul>
</li>
<li>CFG<ul>
<li>Role: CFG is integral for conditional image generation, allowing users to influence the generation based on specific
  conditions.</li>
<li>Explanation: By adjusting the CFG, users can guide the diffusion process to respond to certain prompts, achieving
  conditional outputs aligned with the specified criteria.</li>
</ul>
</li>
<li>Steps<ul>
<li>Role: Steps determine the number of diffusion steps applied to the image.</li>
<li>Explanation: More steps generally result in a more refined image but require additional computation. Users can
  fine-tune this parameter based on the desired trade-off between quality and computational resources.</li>
</ul>
</li>
<li>Seed<ul>
<li>Role: The seed initializes the randomization process, ensuring reproducibility.</li>
<li>Explanation: Setting a seed allows users to reproduce the same image by maintaining a consistent starting point for
  the random processes involved in the diffusion, facilitating result replication.</li>
</ul>
</li>
<li>Batch size<ul>
<li>Role: Batch size influences the number of samples processed simultaneously.</li>
<li>Explanation: A larger batch size can expedite computation but may require more memory. It impacts the efficiency of
  the Stable Diffusion process, with users adjusting it based on available resources and computational preferences.</li>
</ul>
</li>
<li>Prompt<ul>
<li>Role: The prompt provides the textual or visual input guiding the image generation.</li>
<li>Explanation: It serves as the creative input for the algorithm, shaping the direction of the generated content.
  Users articulate their artistic vision or preferences through carefully crafted prompts.</li>
</ul>
</li>
<li>Negative prompt<ul>
<li>Role: Negative prompts offer a counterbalance to positive prompts, influencing the generation towards desired
  qualities or away from specific characteristics.</li>
<li>Explanation: By including a negative prompt, users can fine-tune the generated output, steering it away from
  undesired elements or towards a more nuanced and controlled result.</li>
</ul>
</li>
<li>Width, height<ul>
<li>Role: These parameters define the dimensions of the generated image.</li>
<li>Explanation: Users specify the width and height to control the resolution and aspect ratio of the output. This
  allows for customization based on the intended use or artistic preferences.</li>
</ul>
</li>
</ul>
<h3 id="unique-image-parameters">Unique image parameters</h3>
<p>These parameters are unique to how onnx-web generates images.</p>
<ul>
<li>UNet tile size<ul>
<li>One such parameter is the UNet tile size. This parameter governs the maximum size for each instance the UNet model
  runs. While it aids in reducing memory usage during panoramas and high-resolution processes, caution is needed.
  Reducing this below the image size in txt2img mode can result in repeated "totem pole" bodies, highlighting the
  importance of aligning the tile size appropriately with the intended use case.</li>
</ul>
</li>
<li>UNet overlap<ul>
<li>The UNet overlap parameter plays a pivotal role in determining how much UNet tiles overlap. For most high-resolution
  applications, a value of 0.25 is recommended. However, for larger panoramas, opting for values between 0.5 to 0.75
  seamlessly blends tiles, significantly enhancing panorama quality. Balancing this parameter ensures optimal
  performance in diverse scenarios.</li>
</ul>
</li>
<li>Tiled VAE<ul>
<li>For users engaging with the tiled VAE parameter, the choice revolves around whether the VAE (Variational
  Autoencoder) operates on the entire image or in smaller tiles. Opting for the tiled VAE not only accommodates larger
  images but also reduces VRAM usage. Notably, it doesn't exert a substantial impact on image quality, making it a
  pragmatic choice for scenarios where resource efficiency is a priority.</li>
</ul>
</li>
<li>VAE tile size<ul>
<li>Parallel to UNet, the VAE (Variational Autoencoder) introduces two additional parameters: VAE tile size and VAE
  overlap. These mirror the UNet tile size and UNet overlap parameters, applying specifically to the VAE when the
  tiled VAE is active. Careful consideration of these parameters ensures effective utilization of onnx-web's
  capabilities while adapting to the unique requirements of your image generation tasks.</li>
</ul>
</li>
<li>VAE overlap<ul>
<li>The amount of overlap between each VAE tile.</li>
</ul>
</li>
</ul>
<p>See the complete user guide for details about the highres, upscale, and correction parameters.</p>
<h2 id="prompt-syntax">Prompt syntax</h2>
<p>Constructing prompts in onnx-web closely resembles familiar tools like auto1111, maintaining a consistent syntax
enclosed within angle brackets (<code>&lt; &gt;</code>). Notable differences arise, particularly in the handling of embeddings (Textual
Inversions) and regions. Despite these nuances, users transitioning from other tools will find the syntax comfortably
familiar, allowing for a seamless integration into onnx-web's creative environment.</p>
<h3 id="loras-and-embeddings">LoRAs and embeddings</h3>
<p><code>&lt;lora:filename:1.0&gt;</code> and <code>&lt;embeddings:filename:1.0&gt;</code> or <code>&lt;inversion:filename:1.0&gt;</code>.</p>
<p>You can include additional networks like LoRAs and embeddings (also known as Textual Inversions) using their respective
tokens. The syntax involves specifying the network type, name, and weight, separated by colons. For instance,
<lora:name:weight>. Most LoRA networks exhibit optimal performance within a strength range of 0.8 to 1.0. However, some
networks can be effectively utilized at higher or lower values, spanning from -1.0 to approximately 5.0 for certain
sliders. This flexibility provides users with nuanced control over the influence of LoRA networks on the generated
images.</p>
<p>For more details, please see <a href="../user-guide/#lora-and-lycoris-tokens">the user guide</a>.</p>
<h3 id="clip-skip">CLIP skip</h3>
<p><code>&lt;clip:skip:2&gt;</code> for anime.</p>
<p>onnx-web supports CLIP skip through a token, allowing users to efficiently skip later stages in the CLIP keyword
hierarchy. This feature proves valuable, especially in genres like anime, where skipping specified levels can enhance
image results. For instance, skipping 2 levels refines the output by bypassing stages in the keyword hierarchy,
optimizing the creative outcome. This combination of tokens and functionalities enables users to precisely tailor
prompts in onnx-web for expressive image generation.</p>
<p>For more details, please see <a href="../user-guide/#clip-skip-tokens">the user guide</a>.</p>
<h2 id="highres">Highres</h2>
<p>onnx-web supports a unique highres implementation, a powerful tool for super-resolution upscaling followed by img2img
refinement to restore intricate details. This feature can be iteratively applied to exponentially increase image
resolution. For instance, using a 2x upscaling model, two iterations of highres can produce an image four times the
original size, while three iterations result in an image eight times the size, and so forth.</p>
<p>The technique employed in highres draws parallels to the hires fix seen in other tools but uses repeated img2img runs,
similar to what SDXL does with the refiner. In this approach, the diffusion model refines its own output by running
img2img on smaller tiles, correcting the errors introduced by most super-resolution models. This technique allows
repeated iterative refinement, though it introduces a unique consideration â€“ the img2img tiles do not perceive the
entire image context. Consequently, using a more generic prompt during the highres iterations can be will help avoid
recursive image features.</p>
<h3 id="highres-prompt">Highres prompt</h3>
<p><code>txt2img prompt || img2img prompt</code></p>
<p>Highres prompts are separated from the base txt2img prompt using the double pipe syntax (<code>||</code>). These prompts guide the
upscaling and refinement processes, enabling users to incorporate distinct instructions and achieve nuanced outputs.</p>
<p>For more details, please see <a href="../user-guide/#prompt-stages">the user guide</a>.</p>
<h3 id="highres-iterations">Highres iterations</h3>
<p><code>scale ** iterations</code></p>
<p>Highres mode's iterative approach involves refining the generated image through multiple iterations, each contributing
to an exponential increase in resolution. Users can specify the number of iterations based on their desired level of
refinement. For instance, using a 2x upscaling model, two iterations of Highres will result in an image four times the
original size. This scaling effect continues exponentially with each additional iteration. The interplay of Highres
prompts and iterations allows users to progressively enhance image resolution while maintaining detailed and refined
visual elements.</p>
<p>For more details, please see <a href="../user-guide/#highres-iterations-parameter">the user guide</a>.</p>
<h2 id="profiles">Profiles</h2>
<p>The onnx-web web UI simplifies user experience with the introduction of a feature known as profiles. When you find
your favorite combination of image parameters, you can save them for future use as a named profiled. This is a great
way to reuse successful settings later and streamline your image generation process.</p>
<h3 id="loading-from-files">Loading from files</h3>
<p>Profiles can be loaded directly from image metadata or imported from JSON files shared by other users. Leveraging this
functionality, users can effortlessly reproduce specific parameters and settings that have been fine-tuned by their
peers. This collaborative approach not only streamlines the configuration process but also fosters a community-driven
exchange of preferences and techniques. Whether shared through image metadata or JSON files, loading profiles in
onnx-web offers users a convenient way to benefit from the collective insights and optimizations of the onnx-web
community.</p>
<h3 id="saving-profiles-in-the-web-ui">Saving profiles in the web UI</h3>
<p>The process of saving profiles involves capturing the entire set of image parameters you've fine-tuned to achieve the
desired results. Once saved, these profiles are conveniently accessible for later usage. An additional benefit is the
ability to download your profiles as a JSON snippet. This JSON representation encapsulates the intricacies of your
selected parameters, allowing you to easily share your preferred configurations with others on platforms like Discord
or various online forums.</p>
<h3 id="sharing-parameters-profiles">Sharing parameters profiles</h3>
<p>onnx-web's profile feature extends further by facilitating the loading of parameters from both images and JSON files
containing a profile. This means you can not only share your meticulously crafted settings but also import parameters
from others, enabling collaborative exploration and knowledge exchange within the onnx-web community. As a
user-friendly tool, onnx-web strives to enhance the customization and sharing aspects of image generation, providing
users with a flexible and collaborative experience.</p>
<p>TODO: link Discord profiles channel</p>
<h2 id="panorama-pipeline">Panorama pipeline</h2>
<p>onnx-web introduces a versatile panorama pipeline available for both SD v1.5 and SDXL, offering the flexibility to
generate images without strict size limitations. Users can leverage this pipeline to produce images of 40 megapixels
or larger, reaching dimensions such as 4k by 10k.</p>
<p>The panorama pipeline operates by repeatedly running the original txt2img or img2img pipeline on numerous overlapping
tiles within the image. A deliberate spiral pattern is employed, as it has demonstrated superior results compared to a
grid arrangement. Depending on the chosen degree of tile overlap, users can achieve a completely seamless image.
However, it's essential to note that increased overlap correlates with longer processing times, requiring a thoughtful
balance based on user priorities.</p>
<p>The panorama pipeline in onnx-web thus emerges as a sophisticated tool for users seeking expansive and detailed image
generation. By combining the flexibility of the panorama approach with the precision afforded by region prompts and
seeds, users can delve into intricate compositions and create visually captivating and seamless images.</p>
<h3 id="region-prompts">Region prompts</h3>
<p><code>&lt;region:X:Y:W:H:S:F_TLBR:prompt+&gt;</code></p>
<p>An intriguing feature within the panorama pipeline is the incorporation of region prompts and region seeds. Region
prompts enable users to modify or replace the prompt within specific rectangular regions within the larger image. This
functionality proves invaluable for adding characters to backgrounds, introducing cities into landscapes, or exerting
precise control over where elements appear. It becomes a powerful tool for avoiding crowded or overlapping elements,
offering nuanced control over image composition.</p>
<p>For more details, please see <a href="../user-guide/#region-tokens">the user guide</a>.</p>
<h3 id="region-seeds">Region seeds</h3>
<p><code>&lt;reseed:X:Y:W:H:seed&gt;</code></p>
<p>Furthermore, region seeds enable the replication of the same object or image in different locations, even multiple
times within the same image. To prevent hard edges and seamlessly integrate these region-based modifications, both
region prompts and region seeds include options for blending with the surrounding prompt or seed. It's important to
note that these region features are currently exclusive to the panorama pipeline.</p>
<p>For more details, please see <a href="../user-guide/#reseed-tokens-region-seeds">the user guide</a>.</p>
<h2 id="grid-mode">Grid mode</h2>
<p>onnx-web introduces a powerful feature known as Grid Mode, designed to facilitate the efficient generation of multiple
images with consistent parameters. Once enabled, Grid Mode allows users to select a parameter that varies across
columns and another for rows. Users then provide specific values for each column or row, and the images are generated
by combining the current parameters with the specified column and row values.</p>
<p>In Grid Mode, selecting different parameters for columns and rows ensures diverse variations in the generated images.
It's important to note that the same parameter cannot be chosen for both columns and rows unless the token replacement
option is activated. Token replacement introduces the keywords column and row within the prompt, allowing users to
dynamically insert the column or row values into their prompts before image generation.</p>
<p>While there isn't a strict limit on the number of values users can provide for each dimension (columns and rows), it's
essential to be mindful of the multiplicative effect. The more values provided, the longer the generation process will
take. This trade-off allows users to balance their preferences for quantity and processing time based on their
specific requirements.</p>
<p>Grid Mode offers a versatile and time-efficient approach to exploring variations in image generation, making it a
valuable tool for users seeking a range of outputs with nuanced parameter adjustments. By combining flexibility with
precision, onnx-web empowers users to efficiently navigate the expansive creative possibilities within the Stable
Diffusion process.</p>
<h3 id="grid-tokens">Grid tokens</h3>
<p><code>__column__</code> and <code>__row__</code></p>
<p>When opting for token replacement, users can take advantage of the column and row tokens to dynamically insert column
and row values into their prompts. This feature is particularly powerful when working with comma-separated lists of
options. By utilizing these tokens, you gain the ability to precisely control how parameters evolve across the grid,
facilitating nuanced and controlled variations in image generation.</p>
<h2 id="memory-optimizations">Memory optimizations</h2>
<p>onnx-web introduces optimizations tailored for users with limited memory resources. The system requirements are mostly
based on how much memory you have:</p>
<ul>
<li>minimum requirements<ul>
<li>SD v1.5<ul>
<li>4GB VRAM</li>
<li>8GB system RAM</li>
</ul>
</li>
<li>SDXL<ul>
<li>12GB VRAM</li>
<li>24GB system RAM</li>
</ul>
</li>
</ul>
</li>
<li>recommended requirements<ul>
<li>SD v1.5<ul>
<li>6GB VRAM</li>
<li>16GB system RAM</li>
</ul>
</li>
<li>SDXL<ul>
<li>16GB VRAM</li>
<li>32GB system RAM</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>These optimizations cater to low-memory scenarios, providing users with the flexibility to adapt onnx-web to their
hardware constraints.</p>
<h3 id="converting-to-fp16">Converting to fp16</h3>
<p>The first optimization available is the integration of fp16 mode during model conversion. Employing fp16 significantly
enhances efficiency in terms of storage space and inference, leading to faster runtime for fp16 models. While the
results remain notably similar, it's imperative to acknowledge that not all models are compatible with fp16
conversion, and caution is advised as it may disrupt certain Variational Autoencoders (VAEs).</p>
<h3 id="moving-models-to-the-cpu">Moving models to the CPU</h3>
<p>The second optimization caters to low-memory users by providing the option to offload specific models to the CPU.
Notably, the UNet, being the largest model, is the primary candidate for GPU execution due to improved speed. However,
onnx-web strategically offloads the text encoder and VAE to the CPU, recognizing that the text encoder only runs once
at the beginning of each image, and the VAE operates once or twice at the image's outset and conclusion. This
offloading approach proves especially impactful for SDXL, significantly mitigating memory constraints. While
offloading the VAE might slightly affect high-resolution (highres) speed, it becomes a necessary trade-off to
accommodate SDXL highres on certain GPUs with limited memory resources.</p>
<h2 id="adding-your-own-models">Adding your own models</h2>
<p>onnx-web empowers users to seamlessly integrate their own models into the system through the utilization of an
extras.json file. This JSON file serves as a conduit for users to specify additional models, including LoRA networks and
embeddings, enhancing the versatility of onnx-web's capabilities.</p>
<p>Models that are in the models directory and follow the correct naming pattern will be shown in the web UI whether they
are listed in the extras file or not, but including them in the extras file allows you to provide a label for the web UI
and ensures that the hash is included in your output images.</p>
<h3 id="editing-the-extras-file">Editing the extras file</h3>
<p>Begin by creating an extras.json file and defining your models within the designated structure. For instance:</p>
<pre><code class="language-json">{
  &quot;diffusion&quot;: [
    {
      &quot;format&quot;: &quot;safetensors&quot;,
      &quot;name&quot;: &quot;diffusion-sdxl-turbovision-v3-2&quot;,
      &quot;label&quot;: &quot;SDXL - Turbovision v3.2&quot;,
      &quot;source&quot;: &quot;civitai://255474?type=Model&amp;format=SafeTensor&amp;size=pruned&amp;fp=fp16&quot;,
      &quot;pipeline&quot;: &quot;txt2img-sdxl&quot;
    },
    {
      &quot;format&quot;: &quot;safetensors&quot;,
      &quot;name&quot;: &quot;diffusion-sdxl-icbinp-v1-0&quot;,
      &quot;label&quot;: &quot;SDXL - ICBINP&quot;,
      &quot;source&quot;: &quot;civitai://258447?type=Model&amp;format=SafeTensor&amp;size=pruned&amp;fp=fp16&quot;,
      &quot;pipeline&quot;: &quot;txt2img-sdxl&quot;,
      &quot;hash&quot;: &quot;D6FF242DC70FC3DF8F311091FCD9A4DF3FDC1C85FEE2BCA604D4B8218A62378E&quot;
    }
  ],
  &quot;networks&quot;: [
    {
      &quot;format&quot;: &quot;safetensors&quot;,
      &quot;label&quot;: &quot;SDXL - LCM LoRA HuggingFace&quot;,
      &quot;name&quot;: &quot;sdxl-lcm&quot;,
      &quot;source&quot;: &quot;https://huggingface.co/latent-consistency/lcm-lora-sdxl/resolve/main/pytorch_lora_weights.safetensors&quot;,
      &quot;tokens&quot;: [],
      &quot;type&quot;: &quot;lora&quot;
    }
  ]
}
</code></pre>
<p>Save the extras.json file within the designated onnx-web directory. onnx-web will automatically download and convert the
specified models, streamlining the integration process without manual intervention.</p>
<p>Leverage the "label" field in your extras file to provide localized and user-friendly names for each model. These labels
will be seamlessly integrated into the onnx-web web UI, ensuring an intuitive interface for model selection.</p>
<h2 id="more-details">More details</h2>
<p>For more details, please see the other guides:</p>
<ul>
<li><a href="../api/">API specification</a></li>
<li><a href="../chain-pipelines/">custom chain pipelines</a></li>
<li><a href="../server-admin/">server administration</a></li>
<li><a href="../setup-guide/">setup guide</a></li>
<li><a href="../user-guide/">user guide</a></li>
</ul>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../dev-test/" class="btn btn-neutral float-left" title="Development and Testing"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../server-admin/" class="btn btn-neutral float-right" title="Server Administration">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
        <span>
          <a href="https://github.com/ssube/onnx-web" class="fa fa-github" style="color: #fcfcfc"> GitHub</a>
        </span>
    
    
      <span><a href="../dev-test/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../server-admin/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
